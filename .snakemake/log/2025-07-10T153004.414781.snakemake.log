None
host: login-2
Building DAG of jobs...
Need to rerun job get_insert_domains because of missing output required by intersect_with_uniref50.
Need to rerun job count_caths because job intersect_with_uniref50 has to be rerun.
Need to rerun job popular_inserts because job intersect_with_uniref50 has to be rerun.
Need to rerun job all because job intersect_with_uniref50 has to be rerun.
Need to rerun job intersect_with_uniref50 because job get_insert_domains has to be rerun.
Need to rerun job all because job get_insert_domains has to be rerun.
Need to rerun job all because job count_caths has to be rerun.
Need to rerun job all because job popular_inserts has to be rerun.
outputs/nested_domain_pairs_validations.tsv: True 0
outputs/uniref50_nested_domain_pairs.tsv outputs/unmapped_uniprot_ids.txt outputs/uniref50_frequency.csv: False 1
outputs/insert_cath_counts.csv outputs/parent_cath_counts.csv: False 1
outputs/popular_inserts.csv: False 1
: False 4
shared_storage_local_copies: True
remote_exec: False
Submitting maximum 100 job(s) over 1.0 second(s).
SLURM run ID: 9b908498-2e5b-4122-bd67-d972260cfdff
Using shell: /usr/bin/bash
Provided remote nodes: 5
Job stats:
job                        count
-----------------------  -------
all                            1
count_caths                    1
get_insert_domains             1
intersect_with_uniref50        1
popular_inserts                1
total                          5

Resources before job selection: {'_cores': 9223372036854775807, '_nodes': 5, '_job_count': 9223372036854775807}
Ready jobs: 1
Select jobs to execute...
Selecting jobs to run using greedy solver.
Selected jobs: 1
Resources after job selection: {'_cores': 9223372036854775806, '_nodes': 4, '_job_count': 100}
Execute 1 jobs...
[Thu Jul 10 15:30:04 2025]
rule get_insert_domains:
    input: /home/gridsan/akolodziej/TED/ted_365m.domain_summary.cath.globularity.taxid.tsv
    output: outputs/nested_domain_pairs_validations.tsv
    log: logs/get_insert_domains.log
    jobid: 2
    reason: Missing output files: outputs/nested_domain_pairs_validations.tsv
    resources: tmpdir=<TBD>, mem_mb=32000, mem_mib=30518, time_min=120, cpus_per_task=1
Shell command: python -u scripts/get_insert_domains.py /home/gridsan/akolodziej/TED/ted_365m.domain_summary.cath.globularity.taxid.tsv outputs/nested_domain_pairs_validations.tsv &> logs/get_insert_domains.log
No SLURM account given, trying to guess.
Unable to guess SLURM account. Trying to proceed without.
No wall time information given. This might or might not work on your cluster. If not, specify the resource runtime in your rule or as a reasonable default via --default-resources.
General args: ['--force', '--target-files-omit-workdir-adjustment', '', '--max-inventory-time 0', '--nocolor', '--notemp', '--no-hooks', '--nolock', '--ignore-incomplete', '', '--verbose ', '--rerun-triggers mtime params software-env code input', '', '', '', '', "--conda-frontend 'conda'", '', '', '', '', '', '--shared-fs-usage source-cache storage-local-copies sources input-output software-deployment persistence', '', "--wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/'", '', '', '', '--printshellcmds ', '', '--latency-wait 5', "--scheduler 'ilp'", '', '--local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl', "--scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin'", '', '', '', '', '', '', '--default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI=', '']
sbatch call: sbatch --parsable --job-name 9b908498-2e5b-4122-bd67-d972260cfdff --output "/home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_get_insert_domains/%j.log" --export=ALL --comment "rule_get_insert_domains"   -p xeon-g6-volta --mem 32000 --ntasks=1 --cpus-per-task=1 -D '/home/gridsan/akolodziej/01_insert_domains' --wrap="/home/gridsan/akolodziej/.conda/envs/snakemake/bin/python3.12 -m snakemake --snakefile '/home/gridsan/akolodziej/01_insert_domains/Snakefile' --target-jobs 'get_insert_domains:' --allowed-rules get_insert_domains --cores 'all' --attempt 1 --force-use-threads  --wait-for-files '/home/gridsan/akolodziej/01_insert_domains/.snakemake/tmp.z5x8ousm' '/home/gridsan/akolodziej/TED/ted_365m.domain_summary.cath.globularity.taxid.tsv' --force --target-files-omit-workdir-adjustment --max-inventory-time 0 --nocolor --notemp --no-hooks --nolock --ignore-incomplete --verbose  --rerun-triggers mtime params software-env code input --conda-frontend 'conda' --shared-fs-usage source-cache storage-local-copies sources input-output software-deployment persistence --wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/' --printshellcmds  --latency-wait 5 --scheduler 'ilp' --local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl --scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin' --default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI= --executor slurm-jobstep --jobs 1 --mode 'remote'"
Job 2 has been submitted with SLURM jobid 1187276 (log: /home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_get_insert_domains/1187276.log).
Waiting for running jobs to complete.
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.1707782745361328 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.13315987586975098 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.11832070350646973 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.11175036430358887 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.12187647819519043 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.1045374870300293 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.09968161582946777 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.10440874099731445 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.10698127746582031 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.11191415786743164 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.11912655830383301 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.10448646545410156 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.10987997055053711 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.12218570709228516 seconds
The output is:
'1187276|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.11538338661193848 seconds
The output is:
'1187276|COMPLETED
'

status_of_jobs after sacct is: {'1187276': 'COMPLETED'}
active_jobs_ids_with_current_sacct_status are: {'1187276'}
active_jobs_seen_by_sacct are: {'1187276'}
missing_sacct_status are: set()
removing log for successful job with SLURM ID '1187276'
[Thu Jul 10 15:50:38 2025]
Finished jobid: 2 (Rule: get_insert_domains)
1 of 5 steps (20%) done
outputs/uniref50_nested_domain_pairs.tsv outputs/unmapped_uniprot_ids.txt outputs/uniref50_frequency.csv: True 0
: False 3
Resources before job selection: {'_cores': 9223372036854775807, '_nodes': 5, '_job_count': 100}
Ready jobs: 1
Select jobs to execute...
Selecting jobs to run using greedy solver.
Selected jobs: 1
Resources after job selection: {'_cores': 9223372036854775806, '_nodes': 4, '_job_count': 100}
Execute 1 jobs...
[Thu Jul 10 15:50:39 2025]
rule intersect_with_uniref50:
    input: outputs/uniprot_to_uniref50_mapping.pkl, outputs/nested_domain_pairs_validations.tsv
    output: outputs/uniref50_nested_domain_pairs.tsv, outputs/unmapped_uniprot_ids.txt, outputs/uniref50_frequency.csv
    log: logs/intersect_with_uniref50.log
    jobid: 3
    reason: Input files updated by another job: outputs/nested_domain_pairs_validations.tsv; Set of input files has changed since last execution
    resources: tmpdir=<TBD>, mem_mb=32000, mem_mib=30518, time_min=60, cpus_per_task=1
Shell command: 
        python -u scripts/intersect_nested_domain_pairs.py outputs/uniprot_to_uniref50_mapping.pkl outputs/nested_domain_pairs_validations.tsv outputs/uniref50_nested_domain_pairs.tsv outputs/unmapped_uniprot_ids.txt outputs/uniref50_frequency.csv &> logs/intersect_with_uniref50.log 
        
No wall time information given. This might or might not work on your cluster. If not, specify the resource runtime in your rule or as a reasonable default via --default-resources.
General args: ['--force', '--target-files-omit-workdir-adjustment', '', '--max-inventory-time 0', '--nocolor', '--notemp', '--no-hooks', '--nolock', '--ignore-incomplete', '', '--verbose ', '--rerun-triggers mtime params software-env code input', '', '', '', '', "--conda-frontend 'conda'", '', '', '', '', '', '--shared-fs-usage source-cache storage-local-copies sources input-output software-deployment persistence', '', "--wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/'", '', '', '', '--printshellcmds ', '', '--latency-wait 5', "--scheduler 'ilp'", '', '--local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl', "--scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin'", '', '', '', '', '', '', '--default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI=', '']
sbatch call: sbatch --parsable --job-name 9b908498-2e5b-4122-bd67-d972260cfdff --output "/home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_intersect_with_uniref50/%j.log" --export=ALL --comment "rule_intersect_with_uniref50"   -p xeon-g6-volta --mem 32000 --ntasks=1 --cpus-per-task=1 -D '/home/gridsan/akolodziej/01_insert_domains' --wrap="/home/gridsan/akolodziej/.conda/envs/snakemake/bin/python3.12 -m snakemake --snakefile '/home/gridsan/akolodziej/01_insert_domains/Snakefile' --target-jobs 'intersect_with_uniref50:' --allowed-rules intersect_with_uniref50 --cores 'all' --attempt 1 --force-use-threads  --wait-for-files '/home/gridsan/akolodziej/01_insert_domains/.snakemake/tmp.z5x8ousm' 'outputs/uniprot_to_uniref50_mapping.pkl' 'outputs/nested_domain_pairs_validations.tsv' --force --target-files-omit-workdir-adjustment --max-inventory-time 0 --nocolor --notemp --no-hooks --nolock --ignore-incomplete --verbose  --rerun-triggers mtime params software-env code input --conda-frontend 'conda' --shared-fs-usage source-cache storage-local-copies sources input-output software-deployment persistence --wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/' --printshellcmds  --latency-wait 5 --scheduler 'ilp' --local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl --scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin' --default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI= --executor slurm-jobstep --jobs 1 --mode 'remote'"
Job 3 has been submitted with SLURM jobid 1187868 (log: /home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_intersect_with_uniref50/1187868.log).
Waiting for running jobs to complete.
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.10021448135375977 seconds
The output is:
'1187276|COMPLETED
1187868|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'COMPLETED', '1187868': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187868'}
active_jobs_seen_by_sacct are: {'1187868'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.09985041618347168 seconds
The output is:
'1187276|COMPLETED
1187868|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'COMPLETED', '1187868': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187868'}
active_jobs_seen_by_sacct are: {'1187868'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.10976672172546387 seconds
The output is:
'1187276|COMPLETED
1187868|RUNNING
'

status_of_jobs after sacct is: {'1187276': 'COMPLETED', '1187868': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1187868'}
active_jobs_seen_by_sacct are: {'1187868'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.1225736141204834 seconds
The output is:
'1187276|COMPLETED
1187868|COMPLETED
'

status_of_jobs after sacct is: {'1187276': 'COMPLETED', '1187868': 'COMPLETED'}
active_jobs_ids_with_current_sacct_status are: {'1187868'}
active_jobs_seen_by_sacct are: {'1187868'}
missing_sacct_status are: set()
removing log for successful job with SLURM ID '1187868'
[Thu Jul 10 15:54:19 2025]
Finished jobid: 3 (Rule: intersect_with_uniref50)
2 of 5 steps (40%) done
outputs/insert_cath_counts.csv outputs/parent_cath_counts.csv: True 0
outputs/popular_inserts.csv: True 0
: False 2
Resources before job selection: {'_cores': 9223372036854775807, '_nodes': 5, '_job_count': 100}
Ready jobs: 2
Select jobs to execute...
Selected jobs: 2
Resources after job selection: {'_cores': 9223372036854775805, '_nodes': 3, '_job_count': 100}
Execute 2 jobs...
[Thu Jul 10 15:54:22 2025]
rule count_caths:
    input: outputs/uniref50_nested_domain_pairs.tsv
    output: outputs/insert_cath_counts.csv, outputs/parent_cath_counts.csv
    log: logs/count_caths.log
    jobid: 4
    reason: Input files updated by another job: outputs/uniref50_nested_domain_pairs.tsv
    resources: tmpdir=<TBD>, mem_mb=8000, mem_mib=7630, time_min=120, cpus_per_task=1
Shell command: python -u scripts/count_caths.py outputs/uniref50_nested_domain_pairs.tsv outputs/insert_cath_counts.csv outputs/parent_cath_counts.csv &> logs/count_caths.log
No wall time information given. This might or might not work on your cluster. If not, specify the resource runtime in your rule or as a reasonable default via --default-resources.
General args: ['--force', '--target-files-omit-workdir-adjustment', '', '--max-inventory-time 0', '--nocolor', '--notemp', '--no-hooks', '--nolock', '--ignore-incomplete', '', '--verbose ', '--rerun-triggers mtime params software-env code input', '', '', '', '', "--conda-frontend 'conda'", '', '', '', '', '', '--shared-fs-usage source-cache storage-local-copies sources input-output software-deployment persistence', '', "--wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/'", '', '', '', '--printshellcmds ', '', '--latency-wait 5', "--scheduler 'ilp'", '', '--local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl', "--scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin'", '', '', '', '', '', '', '--default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI=', '']
sbatch call: sbatch --parsable --job-name 9b908498-2e5b-4122-bd67-d972260cfdff --output "/home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_count_caths/%j.log" --export=ALL --comment "rule_count_caths"   -p xeon-g6-volta --mem 8000 --ntasks=1 --cpus-per-task=1 -D '/home/gridsan/akolodziej/01_insert_domains' --wrap="/home/gridsan/akolodziej/.conda/envs/snakemake/bin/python3.12 -m snakemake --snakefile '/home/gridsan/akolodziej/01_insert_domains/Snakefile' --target-jobs 'count_caths:' --allowed-rules count_caths --cores 'all' --attempt 1 --force-use-threads  --wait-for-files '/home/gridsan/akolodziej/01_insert_domains/.snakemake/tmp.z5x8ousm' 'outputs/uniref50_nested_domain_pairs.tsv' --force --target-files-omit-workdir-adjustment --max-inventory-time 0 --nocolor --notemp --no-hooks --nolock --ignore-incomplete --verbose  --rerun-triggers mtime params software-env code input --conda-frontend 'conda' --shared-fs-usage source-cache storage-local-copies sources input-output software-deployment persistence --wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/' --printshellcmds  --latency-wait 5 --scheduler 'ilp' --local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl --scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin' --default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI= --executor slurm-jobstep --jobs 1 --mode 'remote'"
Job 4 has been submitted with SLURM jobid 1188019 (log: /home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_count_caths/1188019.log).
[Thu Jul 10 15:54:22 2025]
rule popular_inserts:
    input: outputs/uniref50_nested_domain_pairs.tsv
    output: outputs/popular_inserts.csv
    log: logs/pop_inserts.log
    jobid: 5
    reason: Input files updated by another job: outputs/uniref50_nested_domain_pairs.tsv
    resources: tmpdir=<TBD>, mem_mb=64000, mem_mib=61036, time_min=180, cpus_per_task=1
Shell command: python -u scripts/popular_inserts.py outputs/uniref50_nested_domain_pairs.tsv outputs/popular_inserts.csv &> logs/pop_inserts.log
No wall time information given. This might or might not work on your cluster. If not, specify the resource runtime in your rule or as a reasonable default via --default-resources.
General args: ['--force', '--target-files-omit-workdir-adjustment', '', '--max-inventory-time 0', '--nocolor', '--notemp', '--no-hooks', '--nolock', '--ignore-incomplete', '', '--verbose ', '--rerun-triggers mtime params software-env code input', '', '', '', '', "--conda-frontend 'conda'", '', '', '', '', '', '--shared-fs-usage source-cache storage-local-copies sources input-output software-deployment persistence', '', "--wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/'", '', '', '', '--printshellcmds ', '', '--latency-wait 5', "--scheduler 'ilp'", '', '--local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl', "--scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin'", '', '', '', '', '', '', '--default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI=', '']
sbatch call: sbatch --parsable --job-name 9b908498-2e5b-4122-bd67-d972260cfdff --output "/home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_popular_inserts/%j.log" --export=ALL --comment "rule_popular_inserts"   -p xeon-g6-volta --mem 64000 --ntasks=1 --cpus-per-task=1 -D '/home/gridsan/akolodziej/01_insert_domains' --wrap="/home/gridsan/akolodziej/.conda/envs/snakemake/bin/python3.12 -m snakemake --snakefile '/home/gridsan/akolodziej/01_insert_domains/Snakefile' --target-jobs 'popular_inserts:' --allowed-rules popular_inserts --cores 'all' --attempt 1 --force-use-threads  --wait-for-files '/home/gridsan/akolodziej/01_insert_domains/.snakemake/tmp.z5x8ousm' 'outputs/uniref50_nested_domain_pairs.tsv' --force --target-files-omit-workdir-adjustment --max-inventory-time 0 --nocolor --notemp --no-hooks --nolock --ignore-incomplete --verbose  --rerun-triggers mtime params software-env code input --conda-frontend 'conda' --shared-fs-usage source-cache storage-local-copies sources input-output software-deployment persistence --wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/' --printshellcmds  --latency-wait 5 --scheduler 'ilp' --local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl --scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin' --default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI= --executor slurm-jobstep --jobs 1 --mode 'remote'"
Job 5 has been submitted with SLURM jobid 1188020 (log: /home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_popular_inserts/1188020.log).
Waiting for running jobs to complete.
Checking the status of 2 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-08T15:00 --endtime now --name 9b908498-2e5b-4122-bd67-d972260cfdff
It took: 0.09997105598449707 seconds
The output is:
'1187276|COMPLETED
1187868|COMPLETED
1188019|COMPLETED
1188020|COMPLETED
'

status_of_jobs after sacct is: {'1187276': 'COMPLETED', '1187868': 'COMPLETED', '1188019': 'COMPLETED', '1188020': 'COMPLETED'}
active_jobs_ids_with_current_sacct_status are: {'1188019', '1188020'}
active_jobs_seen_by_sacct are: {'1188019', '1188020'}
missing_sacct_status are: set()
removing log for successful job with SLURM ID '1188019'
[Thu Jul 10 15:54:59 2025]
Finished jobid: 4 (Rule: count_caths)
3 of 5 steps (60%) done
: False 1
removing log for successful job with SLURM ID '1188020'
[Thu Jul 10 15:54:59 2025]
Finished jobid: 5 (Rule: popular_inserts)
4 of 5 steps (80%) done
: True 0
Resources before job selection: {'_cores': 9223372036854775807, '_nodes': 5, '_job_count': 100}
Ready jobs: 1
Select jobs to execute...
Selecting jobs to run using greedy solver.
Selected jobs: 1
Resources after job selection: {'_cores': 9223372036854775806, '_nodes': 4, '_job_count': 100}
Execute 1 jobs...
[Thu Jul 10 15:54:59 2025]
localrule all:
    input: outputs/uniprot_to_uniref50_mapping.pkl, outputs/nested_domain_pairs_validations.tsv, outputs/uniref50_nested_domain_pairs.tsv, outputs/insert_cath_counts.csv, outputs/popular_inserts.csv
    jobid: 0
    reason: Input files updated by another job: outputs/uniref50_nested_domain_pairs.tsv, outputs/nested_domain_pairs_validations.tsv, outputs/insert_cath_counts.csv, outputs/popular_inserts.csv
    resources: tmpdir=/tmp
Shell command: None
Waiting for running jobs to complete.
[Thu Jul 10 15:54:59 2025]
Finished jobid: 0 (Rule: all)
5 of 5 steps (100%) done
Cleaning up log files older than 10 day(s).
Complete log(s): /home/gridsan/akolodziej/01_insert_domains/.snakemake/log/2025-07-10T153004.414781.snakemake.log
unlocking
removing lock
removing lock
removed all locks
