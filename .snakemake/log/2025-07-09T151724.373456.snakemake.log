None
host: login-2
Building DAG of jobs...
Need to rerun job count_caths because of missing output required by all.
Need to rerun job all because job count_caths has to be rerun.
outputs/insert_cath_counts.csv outputs/parent_cath_counts.csv: True 0
: False 1
shared_storage_local_copies: True
remote_exec: False
Submitting maximum 100 job(s) over 1.0 second(s).
SLURM run ID: 5962cbfd-f6bd-46cc-ae99-104c389b959e
Using shell: /usr/bin/bash
Provided remote nodes: 5
Job stats:
job            count
-----------  -------
all                1
count_caths        1
total              2

Resources before job selection: {'_cores': 9223372036854775807, '_nodes': 5, '_job_count': 9223372036854775807}
Ready jobs: 1
Select jobs to execute...
Selecting jobs to run using greedy solver.
Selected jobs: 1
Resources after job selection: {'_cores': 9223372036854775806, '_nodes': 4, '_job_count': 100}
Execute 1 jobs...
[Wed Jul  9 15:17:24 2025]
rule count_caths:
    input: outputs/uniref50_nested_domain_pairs.tsv
    output: outputs/insert_cath_counts.csv, outputs/parent_cath_counts.csv
    log: logs/count_caths.log
    jobid: 4
    reason: Missing output files: outputs/insert_cath_counts.csv
    resources: tmpdir=<TBD>, mem_mb=8000, mem_mib=7630, time_min=120, cpus_per_task=1
Shell command: python -u scripts/count_caths.py outputs/uniref50_nested_domain_pairs.tsv outputs/insert_cath_counts.csv outputs/parent_cath_counts.csv &> logs/count_caths.log
No SLURM account given, trying to guess.
Unable to guess SLURM account. Trying to proceed without.
No wall time information given. This might or might not work on your cluster. If not, specify the resource runtime in your rule or as a reasonable default via --default-resources.
General args: ['--force', '--target-files-omit-workdir-adjustment', '', '--max-inventory-time 0', '--nocolor', '--notemp', '--no-hooks', '--nolock', '--ignore-incomplete', '', '--verbose ', '--rerun-triggers input software-env mtime params code', '', '', '', '', "--conda-frontend 'conda'", '', '', '', '', '', '--shared-fs-usage storage-local-copies input-output persistence source-cache sources software-deployment', '', "--wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/'", '', '', '', '--printshellcmds ', '', '--latency-wait 5', "--scheduler 'ilp'", '', '--local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl', "--scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin'", '', '', '', '', '', '', '--default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI=', '']
sbatch call: sbatch --parsable --job-name 5962cbfd-f6bd-46cc-ae99-104c389b959e --output "/home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_count_caths/%j.log" --export=ALL --comment "rule_count_caths"   -p xeon-g6-volta --mem 8000 --ntasks=1 --cpus-per-task=1 -D '/home/gridsan/akolodziej/01_insert_domains' --wrap="/home/gridsan/akolodziej/.conda/envs/snakemake/bin/python3.12 -m snakemake --snakefile '/home/gridsan/akolodziej/01_insert_domains/Snakefile' --target-jobs 'count_caths:' --allowed-rules count_caths --cores 'all' --attempt 1 --force-use-threads  --wait-for-files '/home/gridsan/akolodziej/01_insert_domains/.snakemake/tmp.5v0nciqd' 'outputs/uniref50_nested_domain_pairs.tsv' --force --target-files-omit-workdir-adjustment --max-inventory-time 0 --nocolor --notemp --no-hooks --nolock --ignore-incomplete --verbose  --rerun-triggers input software-env mtime params code --conda-frontend 'conda' --shared-fs-usage storage-local-copies input-output persistence source-cache sources software-deployment --wrapper-prefix 'https://github.com/snakemake/snakemake-wrappers/raw/' --printshellcmds  --latency-wait 5 --scheduler 'ilp' --local-storage-prefix base64//LnNuYWtlbWFrZS9zdG9yYWdl --scheduler-solver-path '/home/gridsan/akolodziej/.conda/envs/snakemake/bin' --default-resources base64//dG1wZGlyPXN5c3RlbV90bXBkaXI= --executor slurm-jobstep --jobs 1 --mode 'remote'"
Job 4 has been submitted with SLURM jobid 1154136 (log: /home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_count_caths/1154136.log).
Waiting for running jobs to complete.
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-07T15:00 --endtime now --name 5962cbfd-f6bd-46cc-ae99-104c389b959e
It took: 0.11433219909667969 seconds
The output is:
'1154136|RUNNING
'

status_of_jobs after sacct is: {'1154136': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1154136'}
active_jobs_seen_by_sacct are: {'1154136'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-07T15:00 --endtime now --name 5962cbfd-f6bd-46cc-ae99-104c389b959e
It took: 0.10032916069030762 seconds
The output is:
'1154136|RUNNING
'

status_of_jobs after sacct is: {'1154136': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1154136'}
active_jobs_seen_by_sacct are: {'1154136'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-07T15:00 --endtime now --name 5962cbfd-f6bd-46cc-ae99-104c389b959e
It took: 0.09998106956481934 seconds
The output is:
'1154136|RUNNING
'

status_of_jobs after sacct is: {'1154136': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1154136'}
active_jobs_seen_by_sacct are: {'1154136'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-07T15:00 --endtime now --name 5962cbfd-f6bd-46cc-ae99-104c389b959e
It took: 0.10263252258300781 seconds
The output is:
'1154136|RUNNING
'

status_of_jobs after sacct is: {'1154136': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1154136'}
active_jobs_seen_by_sacct are: {'1154136'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-07T15:00 --endtime now --name 5962cbfd-f6bd-46cc-ae99-104c389b959e
It took: 0.12081432342529297 seconds
The output is:
'1154136|RUNNING
'

status_of_jobs after sacct is: {'1154136': 'RUNNING'}
active_jobs_ids_with_current_sacct_status are: {'1154136'}
active_jobs_seen_by_sacct are: {'1154136'}
missing_sacct_status are: set()
Checking the status of 1 active jobs with 5 attempts.
The job status was queried with command: sacct -X --parsable2 --clusters all --noheader --format=JobIdRaw,State --starttime 2025-07-07T15:00 --endtime now --name 5962cbfd-f6bd-46cc-ae99-104c389b959e
It took: 0.09971475601196289 seconds
The output is:
'1154136|FAILED
'

status_of_jobs after sacct is: {'1154136': 'FAILED'}
active_jobs_ids_with_current_sacct_status are: {'1154136'}
active_jobs_seen_by_sacct are: {'1154136'}
missing_sacct_status are: set()
[Wed Jul  9 15:21:25 2025]
Error in rule count_caths:
    message: SLURM-job '1154136' failed, SLURM status is: 'FAILED'. For further error details see the cluster/cloud log and the log files of the involved rule(s).
    jobid: 4
    input: outputs/uniref50_nested_domain_pairs.tsv
    output: outputs/insert_cath_counts.csv, outputs/parent_cath_counts.csv
    log: logs/count_caths.log, /home/gridsan/akolodziej/01_insert_domains/.snakemake/slurm_logs/rule_count_caths/1154136.log (check log file(s) for error details)
    shell:
        python -u scripts/count_caths.py outputs/uniref50_nested_domain_pairs.tsv outputs/insert_cath_counts.csv outputs/parent_cath_counts.csv &> logs/count_caths.log
        (command exited with non-zero exit code)
    external_jobid: 1154136
Shutting down, this might take some time.
Cleaning up log files older than 10 day(s).
Exiting because a job execution failed. Look below for error messages
[Wed Jul  9 15:22:35 2025]
Error in rule count_caths:
    message: None
    jobid: 4
    input: outputs/uniref50_nested_domain_pairs.tsv
    output: outputs/insert_cath_counts.csv, outputs/parent_cath_counts.csv
    log: logs/count_caths.log (check log file(s) for error details)
    shell:
        python -u scripts/count_caths.py outputs/uniref50_nested_domain_pairs.tsv outputs/insert_cath_counts.csv outputs/parent_cath_counts.csv &> logs/count_caths.log
        (command exited with non-zero exit code)
Complete log(s): /home/gridsan/akolodziej/01_insert_domains/.snakemake/log/2025-07-09T151724.373456.snakemake.log
unlocking
removing lock
removing lock
removed all locks
Full Traceback (most recent call last):
  File "/home/gridsan/akolodziej/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/cli.py", line 2164, in args_to_api
    dag_api.execute_workflow(
  File "/home/gridsan/akolodziej/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/api.py", line 603, in execute_workflow
    workflow.execute(
  File "/home/gridsan/akolodziej/.conda/envs/snakemake/lib/python3.12/site-packages/snakemake/workflow.py", line 1451, in execute
    raise WorkflowError("At least one job did not complete successfully.")
snakemake_interface_common.exceptions.WorkflowError: At least one job did not complete successfully.

WorkflowError:
At least one job did not complete successfully.
